x-healthcheck-options: &healthcheck-options
  interval: 10s
  timeout: 3s
  retries: 3
  start_period: 30s

x-loki-logging: &loki-logging
  driver: loki
  options:
    loki-url: "http://localhost:3100/loki/api/v1/push"
    loki-timeout: "1s"
    loki-retries: "2"

services:
  app:
    environment:
      MONITORING_API_ENABLED: on
      MONITORING_JVM_ENABLED: on
      MONITORING_DBPOOL_ENABLED: on
      MONITORING_HIBERNATE_ENABLED: on
      MONITORING_UPTIME_ENABLED: on
      MONITORING_CPU_ENABLED: on
    networks:
      - frontend
      - application
      - database
      - monitoring
    logging: *loki-logging

  update-admin-password:
    networks:
      - database
    logging: *loki-logging

  create-monitoring-user:
    image: alpine:3.22
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    depends_on:
      update-admin-password:
        condition: service_completed_successfully
    volumes:
      - ./scripts/create-monitoring-user.sh:/create-monitoring-user.sh:ro
    entrypoint: [ "/bin/sh", "/create-monitoring-user.sh" ]
    environment:
      # This must match the name of the Docker Compose service running the DHIS2 app and the port it is listening on
      DHIS2_HOSTNAME: ${DHIS2_HOSTNAME:-http://app:8080}
      DHIS2_ADMIN_USERNAME: ${DHIS2_ADMIN_USERNAME}
      DHIS2_ADMIN_PASSWORD: ${DHIS2_ADMIN_PASSWORD}
      DHIS2_MONITOR_USERNAME: ${DHIS2_MONITOR_USERNAME}
      DHIS2_MONITOR_PASSWORD: ${DHIS2_MONITOR_PASSWORD}
    networks:
      - application
    logging: *loki-logging

  database:
    networks:
      - database
    logging: *loki-logging

  grafana:
    image: grafana/grafana:${GRAFANA_VERSION:-10.0.0}
    volumes:
      - grafana:/var/lib/grafana
      - ./overlays/monitoring/config/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - ./overlays/monitoring/config/grafana/provisioning:/etc/grafana/provisioning:ro
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_ADMIN_USER: admin
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_DISABLE_GRAVATAR: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_LOG_LEVEL: warn
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: https://grafana.${APP_HOSTNAME}
      GF_SERVER_SERVE_FROM_SUB_PATH: false
      DS_PROMETHEUS: prometheus
    networks:
      - frontend
      - monitoring
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:3000/api/health" ]
      <<: *healthcheck-options
    user: 472:472
    cap_drop:
      - ALL
    read_only: true
    security_opt:
      - no-new-privileges:true

  loki:
    image: grafana/loki:${LOKI_VERSION:-2.9.0}
    depends_on:
      loki-init:
        condition: service_completed_successfully
    volumes:
      - loki:/loki
    environment:
      LOKI_RETENTION_PERIOD: ${LOKI_RETENTION_PERIOD:-744h}
    # The Docker Logging Driver is running on the host network, and therefore we need to expose Loki on it
    ports:
      - "0.0.0.0:3100:3100"
    networks:
      - monitoring
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--quiet", "--output-document=/dev/null", "http://localhost:3100/ready" ]
      <<: *healthcheck-options
    user: nobody:nobody
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true

  # This is needed to avoid permission issues with the loki container when running as non-root user
  loki-init:
    image: busybox:1.37.0
    volumes:
      - loki:/loki
    networks:
      - monitoring
    command: [ "sh", "-c", "mkdir -p /loki/chunks /loki/rules /loki/wal /loki/boltdb-shipper-compactor && chown -R nobody:nobody /loki" ]
    user: root

  prometheus:
    image: prom/prometheus:${PROMETHEUS_VERSION:-v2.45.0}
    volumes:
      - prometheus:/prometheus
      - ./overlays/monitoring/config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    secrets:
      - dhis2_monitor_password
    environment:
      PROMETHEUS_RETENTION_TIME: ${PROMETHEUS_RETENTION_TIME:-15d}
    networks:
      - monitoring
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--storage.tsdb.retention.time=15d"
      - "--web.enable-lifecycle"
      - "--web.enable-admin-api"
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--quiet", "--output-document=/dev/null", "http://localhost:9090/-/healthy" ]
      <<: *healthcheck-options
    user: nobody:nobody
    cap_drop:
      - ALL
    # TODO: If we want read only we can't read the DHIS2_MONITOR_PASSWORD from the host environment. We would need to
    #  write it to a file and reference the file in our secrets configuration by the end of this file. For the sake of
    #  simplicity I'll advice that we don't use read only for now.
    #read_only: true
    security_opt:
      - no-new-privileges:true

  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter:${POSTGRES_EXPORTER_VERSION:-v0.17.1}
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_METRICS_USERNAME}:${POSTGRES_METRICS_PASSWORD}@database:5432/${POSTGRES_DB}?sslmode=disable"
    networks:
      - database
      - monitoring
    depends_on:
      create-monitoring-user:
        condition: service_completed_successfully
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--quiet", "--output-document=/dev/null", "http://localhost:9187/metrics" ]
      <<: *healthcheck-options
    user: nobody:nobody
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    logging: *loki-logging

  traefik:
    logging: *loki-logging
    networks:
      - monitoring
      - frontend

  node-exporter:
    image: prom/node-exporter:${NODE_EXPORTER_VERSION:-v1.6.1}
    user: "65534:65534"
    networks:
      - monitoring
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host)($$|/)'
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--quiet", "--output-document=/dev/null", "http://localhost:9100/metrics" ]
      <<: *healthcheck-options

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:${CADVISOR_VERSION:-v0.47.0}
    privileged: true
    userns_mode: "host"
    networks:
      - monitoring
    command:
      # For the sake of avoiding: "Unable to get btrfs mountpoint IDs: stat failed on /dev/mapper/luks-id with error: no such file or directory"
      # https://github.com/google/cadvisor/issues/3357
      - --disable_metrics=disk,referenced_memory
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:rw
      - /sys:/sys:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--quiet", "--output-document=/dev/null", "http://localhost:8080/metrics" ]
      <<: *healthcheck-options

volumes:
  loki: { }
  prometheus: { }
  grafana: { }

secrets:
  dhis2_monitor_password:
    environment: DHIS2_MONITOR_PASSWORD
